import streamlit as st
from PyPDF2 import PdfReader
import os
import time
import json
import re
from google import genai

# --- Imports ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS

# --- ğŸ”‘ API Key ---
FIXED_API_KEY = "AIzaSyDjZ0igflBSazzUC3bEHf3mjLQ1L_E6dkI" 
INDEX_FOLDER = "faiss_index_ae"

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Ø§Ù„Ù…Ø¨Ø§Ø¯Ø± Ø§Ù„Ø°Ø§ØªÙŠ - Assistant", page_icon="ğŸ‡¹ğŸ‡³", layout="centered")

# --- CSS Styling ---
st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body, [class*="css"] {
        font-family: 'Cairo', sans-serif;
        direction: rtl;
        text-align: right;
    }
    .stChatMessage {
        direction: rtl !important;
        text-align: right !important;
        border-radius: 15px;
        padding: 15px;
        font-size: 16px;
    }
    div[data-testid="stChatMessageContent"] {
        direction: rtl !important;
        text-align: right !important;
    }
    .stButton button {
        width: 100%;
        border-radius: 8px;
        background-color: #f0f2f6;
        color: #1f77b4;
        border: 1px solid #d6d6d6;
        font-family: 'Cairo', sans-serif;
        font-weight: bold;
        transition: 0.3s;
    }
    .stButton button:hover {
        background-color: #e2e6ea;
        border-color: #1f77b4;
    }
    h1, h2, h3 { 
        color: #1f77b4; 
        text-align: center; 
        font-family: 'Cairo', sans-serif;
    }
    .stDeployButton {display:none;}
    </style>
""", unsafe_allow_html=True)

# --- Ø§Ù„Ø¯ÙˆØ§Ù„ (Functions) ---

def get_all_files_text(file_list):
    text = ""
    for file_path in file_list:
        try:
            if file_path.endswith('.pdf'):
                pdf_reader = PdfReader(file_path)
                for page in pdf_reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        text += extracted + "\n"
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    text += f.read() + "\n"
        except FileNotFoundError:
            st.warning(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {file_path} Ù…ÙÙ‚ÙˆØ¯.")
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {e}")
    return text

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_text(text)
    return chunks

def create_vector_store_with_batches(text_chunks, api_key):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    vector_store = None
    batch_size = 5
    total_chunks = len(text_chunks)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        for i in range(0, total_chunks, batch_size):
            batch = text_chunks[i:i+batch_size]
            progress = min((i + batch_size) / total_chunks, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: {int(progress*100)}% ...")

            if vector_store is None:
                vector_store = FAISS.from_texts(batch, embedding=embeddings)
            else:
                vector_store.add_texts(batch)
            time.sleep(1)
            
        vector_store.save_local(INDEX_FOLDER)
        status_text.success("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()
        return True
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {e}")
        return False

def get_gemini_response_with_suggestions(context_text, user_question, api_key):
    client = genai.Client(api_key=api_key)
    
    # --- Prompt Ø°ÙƒÙŠ ÙŠØ·Ù„Ø¨ Ø§Ù„Ø¬ÙˆØ§Ø¨ + 3 Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ---
    prompt = f"""
    Ø£Ù†Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù…Ù†ØµØ© "Ø§Ù„Ù…Ø¨Ø§Ø¯Ø± Ø§Ù„Ø°Ø§ØªÙŠ" ÙÙŠ ØªÙˆÙ†Ø³.
    
    Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª):
    {context_text}
    
    Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙˆØ§Ø·Ù†:
    {user_question}
    
    ğŸ”´ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø© (Red Lines):
    1. **Ø§Ù„ØªØ­ÙŠØ©**: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¬Ø±Ø¯ ØªØ­ÙŠØ© (Ø³Ù„Ø§Ù…ØŒ ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±)ØŒ Ø¬Ø§ÙˆØ¨ Ø¨ØªØ±Ø­ÙŠØ¨ ÙÙ‚Ø· ÙˆÙ„Ø§ ØªÙ‚ØªØ±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø¹Ù‚Ø¯Ø©.
    2. **Ø§Ù„Ù„ØºØ©**: Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·.
    3. **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚**: Ø£Ø±ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø´ÙƒÙ„ JSON ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­Ù‚Ù„ÙŠÙ†:
       - "answer": Ù†Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…Ù†Ø³Ù‚ Ø¨Ø®Ø·ÙˆØ§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±).
       - "suggestions": Ù‚Ø§Ø¦Ù…Ø© ÙÙŠÙ‡Ø§ Ø¨Ø§Ù„Ø¶Ø¨Ø· 3 Ø£Ø³Ø¦Ù„Ø© Ù‚ØµÙŠØ±Ø© Ù„Ù‡Ø§ Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ (Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø­ÙˆØ§Ø±).
    4. **Ø§Ù„Ù…Ø­ØªÙˆÙ‰**: Ù„Ø§ ØªØ°ÙƒØ± Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªÙ‚Ù†ÙŠØ© (IHM, Zone).
    
    Ù…Ø«Ø§Ù„ Ù„Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (JSON):
    {{
      "answer": "Ù†Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‡Ù†Ø§...",
      "suggestions": ["Ø³Ø¤Ø§Ù„ Ù…Ù‚ØªØ±Ø­ 1", "Ø³Ø¤Ø§Ù„ Ù…Ù‚ØªØ±Ø­ 2", "Ø³Ø¤Ø§Ù„ Ù…Ù‚ØªØ±Ø­ 3"]
    }}
    
    Ø¬Ø§ÙˆØ¨ Ø§Ù„Ø¢Ù† Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·:
    """
    
    try:
        response = client.models.generate_content(
            model='gemini-flash-latest', # Ù†Ø³ØªØ¹Ù…Ù„Ùˆ Flash Ø¨Ø§Ø´ ÙŠÙƒÙˆÙ† Ø³Ø±ÙŠØ¹ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ JSON
            contents=prompt,
            config={'response_mime_type': 'application/json'} # Ù†Ø¬Ø¨Ø¯Ùˆ JSON ØµØ§ÙÙŠ
        )
        return json.loads(response.text)
    except Exception as e:
        # ÙÙŠ ØµÙˆØ±Ø© Ù…Ø§ ØµØ§Ø± Ø®Ø·Ø£ØŒ Ù†Ø±Ø¬Ø¹Ùˆ Ø¬ÙˆØ§Ø¨ Ø¹Ø§Ø¯ÙŠ ÙˆØ§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¹Ø§Ù…Ø©
        return {
            "answer": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ù…Ø¤Ù‚Øª. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
            "suggestions": ["Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ù†Ø®Ø±Ø§Ø·ØŸ", "ÙƒÙŠÙ Ø£Ø¯ÙØ¹ØŸ", "Ø§ØªØµÙ„ Ø¨Ø§Ù„Ø¯Ø¹Ù…"]
        }

def process_query(user_question, api_key):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    try:
        new_db = FAISS.load_local(INDEX_FOLDER, embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question, k=6)
        context = "\n".join([doc.page_content for doc in docs])
        return get_gemini_response_with_suggestions(context, user_question, api_key)
    except Exception:
        return {
            "answer": "âš ï¸ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ø¬Ø§Ù‡Ø². Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.",
            "suggestions": []
        }

# --- Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Main) ---

def main():
    st.title("ğŸ‡¹ğŸ‡³ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø¨Ø§Ø¯Ø± Ø§Ù„Ø°Ø§ØªÙŠ")
    st.markdown("<p style='text-align: center; color: gray;'>Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ÙƒÙ„ Ù…Ø§ ÙŠØ®Øµ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø¯Ø± Ø§Ù„Ø°Ø§ØªÙŠ</p>", unsafe_allow_html=True)

    # 1. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù€ Session State
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ! ğŸ‘‹\nØ£Ù†Ø§ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù…Ù†ØµØ© Ø§Ù„Ù…Ø¨Ø§Ø¯Ø± Ø§Ù„Ø°Ø§ØªÙŠ.\n\nØªÙØ¶Ù„ØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"}
        ]
    
    # 2. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª (Suggestions)
    if "current_suggestions" not in st.session_state:
        st.session_state.current_suggestions = ["Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ù†Ø®Ø±Ø§Ø·ØŸ", "ÙƒÙŠÙ Ø£Ø¯ÙØ¹ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø§ØªØŸ", "Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ"]

    # --- Sidebar ---
    with st.sidebar:
        st.image("https://www.autoentrepreneur.tn/assets/images/logo-ae.png", width=150)
        st.header("Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        
        if os.path.exists(f"{INDEX_FOLDER}/index.faiss"):
            st.success("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØµÙ„Ø© ğŸŸ¢")
        else:
            st.error("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ğŸ”´")
            
        if st.button("ğŸ”„ Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Entrainement)"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ«..."):
                files_to_process = [
                    "TDRS AE  PHASE 1_07-2024.pdf", 
                    "projet cahier des charges phase II Autoentrepreneur.pdf",
                    "rapport-auto-entrepreneur.pdf",
                    "more_data.txt"
                ]
                existing_files = [f for f in files_to_process if os.path.exists(f)]
                if existing_files:
                    raw_text = get_all_files_text(existing_files)
                    if raw_text:
                        text_chunks = get_text_chunks(raw_text)
                        create_vector_store_with_batches(text_chunks, FIXED_API_KEY)
                        st.rerun()

    # 3. Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 4. Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© (Dynamic Buttons)
    # ØªØ¸Ù‡Ø± ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¢Ø®Ø± Ù…ÙŠØ³Ø§Ø¬ Ù…Ù† Ø¹Ù†Ø¯ Ø§Ù„Ù€ Assistant
    if st.session_state.messages[-1]["role"] == "assistant":
        suggestions = st.session_state.current_suggestions
        if suggestions:
            st.markdown("###### Ø£Ø³Ø¦Ù„Ø© Ù…Ù‚ØªØ±Ø­Ø©:")
            cols = st.columns(len(suggestions))
            for i, suggestion in enumerate(suggestions):
                if cols[i].button(suggestion, key=f"sugg_{len(st.session_state.messages)}_{i}"):
                    handle_user_input(suggestion)

    # 5. Ø®Ø§Ù†Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©
    if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§..."):
        handle_user_input(prompt)

def handle_user_input(prompt):
    # Ø¹Ø±Ø¶ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬ÙˆØ§Ø¨
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©..."):
            # Ù†ØªØ­ØµÙ„Ùˆ Ø¹Ù„Ù‰ Ø§Ù„Ù€ JSON (Ø§Ù„Ø¬ÙˆØ§Ø¨ + Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª)
            result_json = process_query(prompt, FIXED_API_KEY)
            
            full_response = result_json.get("answer", "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø©.")
            new_suggestions = result_json.get("suggestions", [])
            
            message_placeholder.markdown(full_response)
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© (State)
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©
    if new_suggestions:
        st.session_state.current_suggestions = new_suggestions
    else:
        st.session_state.current_suggestions = [] # ØªÙØ±ÙŠØº Ø¥Ø°Ø§ Ù…ÙÙ…Ø§Ø´ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
        
    st.rerun()

if __name__ == "__main__":
    main()